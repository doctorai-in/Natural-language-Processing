{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec-glove-classification.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO9mclkV9n-5"
      },
      "source": [
        "%%bash \n",
        "git clone https://github.com/Eligijus112/twitter-genuine-tweets.git\n",
        "cd twitter-genuine-tweets\n",
        "#wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tbg6Exk-_n7"
      },
      "source": [
        "cd twitter-genuine-tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNvC1_tiK2Ko"
      },
      "source": [
        "# https://nlp.stanford.edu/projects/glove/\n",
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRuePL96NOVx"
      },
      "source": [
        "!unzip glove.840B.300d.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D6H3Q1nHyL2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "     \n",
        "\n",
        "class TextToTensor():\n",
        "\n",
        "    def __init__(self, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def string_to_tensor(self, string_list: list) -> list:\n",
        "        \"\"\"\n",
        "        A method to convert a string list to a tensor for a deep learning model\n",
        "        \"\"\"    \n",
        "        string_list = self.tokenizer.texts_to_sequences(string_list)\n",
        "        print(\"tokenizer_word_sequence : \", string_list[0])\n",
        "        string_list = pad_sequences(string_list, maxlen=self.max_len)\n",
        "        \n",
        "        return string_list\n",
        "\n",
        "def clean_text(\n",
        "    string: str, \n",
        "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
        "    stop_words=[]) -> str:\n",
        "    \"\"\"\n",
        "    A method to clean text \n",
        "    \"\"\"\n",
        "    # Cleaning the urls\n",
        "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
        "\n",
        "    # Cleaning the html elements\n",
        "    string = re.sub(r'<.*?>', '', string)\n",
        "\n",
        "    # Removing the punctuations\n",
        "    for x in string.lower(): \n",
        "        if x in punctuations: \n",
        "            string = string.replace(x, \"\") \n",
        "\n",
        "    # Converting the text to lower\n",
        "    string = string.lower()\n",
        "\n",
        "    # Removing stop words\n",
        "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
        "\n",
        "    # Cleaning the whitespaces\n",
        "    string = re.sub(r'\\s+', ' ', string).strip()\n",
        "\n",
        "    return string        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5HaK_tXIDGf"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Loading the word tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# For accuracy calculations\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "class Pipeline:\n",
        "    \"\"\"\n",
        "    A class for the machine learning pipeline\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, \n",
        "        X_train: list, \n",
        "        Y_train: list, \n",
        "        embed_path: str, \n",
        "        embed_dim: int,\n",
        "        stop_words=[],\n",
        "        X_test=[], \n",
        "        Y_test=[],\n",
        "        max_len=None,\n",
        "        epochs=3,\n",
        "        batch_size=256\n",
        "        ):\n",
        "\n",
        "        # Preprocecing the text\n",
        "        X_train = [clean_text(text, stop_words=stop_words) for text in X_train]\n",
        "        Y_train = np.asarray(Y_train)\n",
        "        \n",
        "        # Tokenizing the text\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "        # Saving the tokenizer\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Creating the embedding matrix\n",
        "        embedding = Embeddings(embed_path, embed_dim)\n",
        "        embedding_matrix = embedding.create_embedding_matrix(tokenizer, len(tokenizer.word_counts))\n",
        "\n",
        "        # Creating the padded input for the deep learning model\n",
        "        if max_len is None:\n",
        "            max_len = np.max([len(text.split()) for text in X_train])\n",
        "        TextToTensor_instance = TextToTensor(\n",
        "            tokenizer=tokenizer, \n",
        "            max_len=max_len\n",
        "            )\n",
        "        X_train = TextToTensor_instance.string_to_tensor(X_train)\n",
        "        print(\"X_train Feature: \", X_train[0] )\n",
        "        print(\"embedding_matrix : \", embedding_matrix[0])\n",
        "        print(\"embedding matrix shape :\", embedding_matrix.shape)\n",
        "        print(\"max_len input to neural network\", max_len)\n",
        "        print(\"Embedding Dimensions : \", embed_dim)\n",
        "        # Creating the model\n",
        "        rnn = RnnModel(\n",
        "            embedding_matrix=embedding_matrix, \n",
        "            embedding_dim=embed_dim, \n",
        "            max_len=max_len\n",
        "        )\n",
        "        rnn.model.fit(\n",
        "            X_train,\n",
        "            Y_train, \n",
        "            batch_size=batch_size, \n",
        "            epochs=epochs\n",
        "        )\n",
        "\n",
        "        self.model = rnn.model\n",
        "\n",
        "        # If X_test is provided we make predictions with the created model\n",
        "        if len(X_test)>0:\n",
        "            X_test = [clean_text(text) for text in X_test]\n",
        "            X_test = TextToTensor_instance.string_to_tensor(X_test)\n",
        "            yhat = [x[0] for x in rnn.model.predict(X_test).tolist()]\n",
        "            \n",
        "            self.yhat = yhat\n",
        "\n",
        "            # If true labels are provided we calculate the accuracy of the model\n",
        "            if len(Y_test)>0:\n",
        "                self.acc = accuracy_score(Y_test, [1 if x > 0.5 else 0 for x in yhat])\n",
        "                self.f1 = f1_score(Y_test, [1 if x > 0.5 else 0 for x in yhat])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELFcV8U5IMPn"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Embeddings():\n",
        "    \"\"\"\n",
        "    A class to read the word embedding file and to create the word embedding matrix\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, vector_dimension):\n",
        "        self.path = path \n",
        "        self.vector_dimension = vector_dimension\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_coefs(word, *arr): \n",
        "        return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "    def get_embedding_index(self):\n",
        "        embeddings_index = dict(self.get_coefs(*o.split(\" \")) for o in open(self.path, errors='ignore'))\n",
        "        return embeddings_index\n",
        "\n",
        "    def create_embedding_matrix(self, tokenizer=None, max_features=None):\n",
        "        \"\"\"\n",
        "        A method to create the embedding matrix\n",
        "        \"\"\"\n",
        "        model_embed = self.get_embedding_index()\n",
        "\n",
        "        if max_features is None:\n",
        "            max_features = len(model_embed)\n",
        "\n",
        "        word_index = model_embed\n",
        "        if tokenizer is not None: \n",
        "            word_index = tokenizer.word_index\n",
        "\n",
        "        embedding_matrix = np.zeros((max_features + 1, self.vector_dimension))\n",
        "        for index, word in enumerate(word_index.keys()):\n",
        "            if index > max_features:\n",
        "                break\n",
        "            else:\n",
        "                try:\n",
        "                    embedding_matrix[index] = model_embed[word]\n",
        "                except:\n",
        "                    continue\n",
        "        return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBgwSJdcIWdy"
      },
      "source": [
        "%%writefile conf.yml\n",
        "pipeline:\n",
        "  k_fold: True\n",
        "  save_results: True\n",
        "  batch_size: 256\n",
        "  epochs: 7\n",
        "  max_len: 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSQ6XU2mIptc"
      },
      "source": [
        "# Deep learning: \n",
        "from keras.models import Input, Model\n",
        "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "\n",
        "class RnnModel():\n",
        "    \"\"\"\n",
        "    A recurrent neural network for semantic analysis\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_matrix, embedding_dim, max_len, X_additional=None):\n",
        "        \n",
        "        inp1 = Input(shape=(max_len,))\n",
        "        x = Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix])(inp1)\n",
        "        x = LSTM(256, return_sequences=True)(x)\n",
        "        x = LSTM(128)(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(64, activation=\"relu\")(x)\n",
        "        x = Dense(1, activation=\"sigmoid\")(x)    \n",
        "        model = Model(inputs=inp1, outputs=x)\n",
        "\n",
        "        model.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfythJfaHptw"
      },
      "source": [
        "# Package for data wrangling\n",
        "import pandas as pd \n",
        "\n",
        "# Package for array math\n",
        "import numpy as np \n",
        "\n",
        "# Package for system path traversal\n",
        "import os\n",
        "\n",
        "# Package for working with dates\n",
        "from datetime import date\n",
        "\n",
        "# K fold analysis package\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Reading the configuration file\n",
        "import yaml\n",
        "with open(\"conf.yml\", 'r') as file:\n",
        "    conf = yaml.safe_load(file).get('pipeline')\n",
        "\n",
        "# Reading the stop words\n",
        "stop_words = []\n",
        "try:\n",
        "    stop_words = pd.read_csv('data/stop_words.txt', sep='\\n', header=None)[0].tolist()\n",
        "except Exception as e:\n",
        "    # This exception indicates that the file is missing or is in a bad format\n",
        "    print('Bad stop_words.txt file: {e}')\n",
        "\n",
        "# Reading the data\n",
        "train = pd.read_csv('data/train.csv')[['text', 'target']]\n",
        "test = pd.read_csv('data/test.csv')\n",
        "\n",
        "# Shuffling the data for the k fold analysis\n",
        "train = train.sample(frac=1)\n",
        "\n",
        "# Creating the input for the pipeline\n",
        "X_train = train['text'].tolist()\n",
        "Y_train = train['target'].tolist()\n",
        "\n",
        "X_test = test['text'].tolist()\n",
        "\n",
        "if conf.get('k_fold'):\n",
        "    kfold = KFold(n_splits=5)\n",
        "    acc = []\n",
        "    f1 = []\n",
        "    for train_index, test_index in kfold.split(X_train):\n",
        "        # Fitting the model and forecasting with a subset of data\n",
        "        k_results = Pipeline(\n",
        "            X_train=np.array(X_train)[train_index],\n",
        "            Y_train=np.array(Y_train)[train_index], \n",
        "            embed_path='glove.840B.300d.txt',\n",
        "            embed_dim=300,\n",
        "            X_test=np.array(X_train)[test_index],\n",
        "            Y_test=np.array(Y_train)[test_index],\n",
        "            max_len=conf.get('max_len'),\n",
        "            epochs=conf.get('epochs'),\n",
        "            batch_size=conf.get('batch_size')\n",
        "        )\n",
        "        # Saving the accuracy\n",
        "        acc += [k_results.acc]\n",
        "        f1 += [k_results.f1]\n",
        "        print(f'The accuracy score is: {acc[-1]}') \n",
        "        print(f'The f1 score is: {f1[-1]}') \n",
        "    print(f'Total mean accuracy is: {np.mean(acc)}')\n",
        "    print(f'Total mean f1 score is: {np.mean(f1)}')\n",
        "\n",
        "# Running the pipeline with all the data\n",
        "results = Pipeline(\n",
        "    X_train=X_train,\n",
        "    Y_train=Y_train, \n",
        "    embed_path='glove.840B.300d.txt',\n",
        "    embed_dim=300,\n",
        "    stop_words=stop_words,\n",
        "    X_test=X_test,\n",
        "    max_len=conf.get('max_len'),\n",
        "    epochs=conf.get('epochs'),\n",
        "    batch_size=conf.get('batch_size')\n",
        ")\n",
        "\n",
        "# Some sanity checks\n",
        "good = [\"Fire in Vilnius! Where is the fire brigade??? #emergency\"]\n",
        "bad = [\"Sushi or pizza? Life is hard :((\"]\n",
        "\n",
        "TextToTensor_instance = TextToTensor(\n",
        "tokenizer=results.tokenizer,\n",
        "max_len=conf.get('max_len')\n",
        ")\n",
        "\n",
        "# Converting to tensors\n",
        "good_nn = TextToTensor_instance.string_to_tensor(good)\n",
        "bad_nn = TextToTensor_instance.string_to_tensor(bad)\n",
        "\n",
        "# Forecasting\n",
        "p_good = results.model.predict(good_nn)[0][0]\n",
        "p_bad = results.model.predict(bad_nn)[0][0]\n",
        "\n",
        "print(f'Sentence: {good_nn} Score: {p_good}')\n",
        "print(f'Sentence: {bad_nn} Score: {p_bad}')\n",
        "\n",
        "# Saving the predictions\n",
        "test['prob_is_genuine'] = results.yhat\n",
        "test['target'] = [1 if x > 0.5 else 0 for x in results.yhat]\n",
        " \n",
        "# Saving the predictions to a csv file\n",
        "if conf.get('save_results'):\n",
        "    if not os.path.isdir('output'):\n",
        "        os.mkdir('output')    \n",
        "    test[['id', 'target']].to_csv(f'output/submission_{date.today()}.csv', index=False)\n",
        "\n",
        "# https://stackoverflow.com/questions/38189713/what-is-an-embedding-in-keras    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}